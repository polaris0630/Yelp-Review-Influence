{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import bspline\n",
    "import bspline.splinelab as splinelab\n",
    "import sys\n",
    "path = r'C:\\Users\\Yichen Jiang\\Documents\\PHD LIFE\\Research\\Hawkes Processes\\Yelp'\n",
    "sys.path.append(path)\n",
    "from yelp_functions import *\n",
    "from scipy.interpolate import splrep, BSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load R session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import business name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Yichen Jiang\\Documents\\PHD LIFE\\Research\\Hawkes Processes\\Yelp'\n",
    "\n",
    "path_business_withtext = os.path.join(path,'business with text features')\n",
    "\n",
    "path_results = os.path.join(path,'results')\n",
    "\n",
    "filenames = os.listdir(path_business_withtext)\n",
    "\n",
    "results = os.listdir(path_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR choose business_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#business_id = '2GmGT-7QjowR1ihup3FbVA' # review number: 825\n",
    "#business_id = '0FUtlsQrJI7LhqDPxLumEw' # review number: 2097\n",
    "#business_id = '2weQS-RnoOBhb1KsHKyoSQ' # review number: 4534\n",
    "#business_id = 'RESDUcs7fIiihp38-d6_6g' # review number: 8568\n",
    "#business_id = 'P7pxQFqr7yBKMMI2J51udw' # review number: 3225\n",
    "#business_id = '--9e1ONYQuAa-CB_Rrw7Tw' # review number: 1661\n",
    "#business_id = '4JNXUYY8wbaaDmk3BPzlWw' # review number: 8570\n",
    "#business_id = 'DkYS3arLOhA8si5uUEmHOw' # review number: 5206\n",
    "#business_id = '5LNZ67Yw9RD6nf4_UhXOjw' # review number: 4522\n",
    "#business_id = 'K7lWdNUhCbcnEvI0NhGewg' # review number: 6887\n",
    "#business_id = 'cYwJA2A6I12KNkm2rtXd5g' # review number: 5575\n",
    "#business_id = 'uW6UHfONAmm8QttPkbMewQ' # review number: 1463\n",
    "#business_id = 'S-oLPRdhlyL5HAknBKTUcQ' # review number: 1139\n",
    "#business_id = 'AV6weBrZFFBfRGCbcRGO4g' # review number: 4240\n",
    "#business_id = 'ebTvBxSStI9Vf5Tpux_X3Q' # review number: 1107\n",
    "business_id = 'J1RDyyPxhioqm8c_fi4P4Q' # review number: 1023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define other terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of deltas\n",
    "list_deltas = [0.005, 0.05, 0.1, 1, 5]\n",
    "\n",
    "# list of variables\n",
    "list_variables = []\n",
    "for i in range(0,5):\n",
    "    list_variables.append(str(i+1)+'star')\n",
    "list_variables += ['cool', 'funny', 'useful', 'average_stars', 'friend_count', 'elite_count', 'review_count', 'fan_count', 'yelping_since', 'mean_prob', 'mean_weight', 'sentiment_polarity', 'sentiment_subjectivity', 'text_length', 'textclean_length']\n",
    "\n",
    "bspline_order = 3\n",
    "\n",
    "knot_base = 'order' #'time' or 'order'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "list_knot_base = ['order','time']\n",
    "list_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = os.listdir(path_results)\n",
    "if 'time'and'order' in list_record:\n",
    "    print(filenames[count].split('_with')[0]+ ' with all basis have been processed')\n",
    "    count += 1\n",
    "    list_record = []\n",
    "if count <= len(filenames):\n",
    "    for base in list_knot_base:\n",
    "        if filenames[count].split('_with')[0]+'_'+str(base)+'.png' in results:\n",
    "            print(filenames[count].split('_with')[0]+'_'+str(base)+'.png has already been processed')\n",
    "            list_record.append(str(base))\n",
    "        elif filenames[count].split('_with')[0]+'_'+str(base)+'.png' not in results:\n",
    "            business_id = filenames[count].split('_with')[0]\n",
    "            knot_base = base\n",
    "            print('business_id is:',str(business_id),', knot_base is:',str(knot_base))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(business_id)\n",
    "print(knot_base)\n",
    "print('the next count is:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    print(filename.split('_with')[0])\n",
    "    # update results\n",
    "    results = os.listdir(path_results)\n",
    "    if filename.endswith('.json'):\n",
    "        if filename.split('_with')[0]+'_'+'order'+'_coefficients.csv' not in results:\n",
    "            knot_base = 'order'\n",
    "            business_id = filename.split('_with')[0]\n",
    "            print(business_id, knot_base)\n",
    "            try:\n",
    "                main(business_id,path,list_deltas,list_variables,bspline_order,knot_base)\n",
    "            except KeyError as error:\n",
    "                print(error)\n",
    "        elif filename.split('_with')[0]+'_'+'order'+'_coefficients.csv' in results:\n",
    "            print(filename.split('_with')[0] + 'in order base has been finished')\n",
    "\n",
    "        if filename.split('_with')[0]+'_'+'time'+'_coefficients.csv' not in results:\n",
    "            knot_base = 'time'\n",
    "            business_id = filename.split('_with')[0]\n",
    "            print(business_id, knot_base)\n",
    "            try:\n",
    "                main(business_id,path,list_deltas,list_variables,bspline_order,knot_base)\n",
    "            except KeyError as error:\n",
    "                print(error)\n",
    "        elif filename.split('_with')[0]+'_'+'time'+'_coefficients.csv' in results:\n",
    "            print(filename.split('_with')[0] + 'in time base has been finished')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename.split('_with')[0]+'_'+'order'+'_coefficients.csv' not in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename.split('_with')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lookup dictionary and table\n",
    "df_decaytable, dict_decaytable = create_decay_table(list_deltas)\n",
    "\n",
    "# create dict_calendar for time-checking\n",
    "dict_calendar = create_calendar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(business_id,path,list_deltas,list_variables,bspline_order,knot_base):\n",
    "    global dict_business\n",
    "    global df_business\n",
    "    global df_variables\n",
    "    global df_bspline\n",
    "    global dict_effective\n",
    "    global dict_decayvalues\n",
    "    global df_decaytable\n",
    "    global dict_decaytable\n",
    "    global dict_calendar\n",
    "    global df_data\n",
    "    global list_knot_order\n",
    "    global list_knot_time\n",
    "    \n",
    "    # original business information (with user and text features)\n",
    "    dict_business = {}\n",
    "    path_user = os.path.join(path,'business','business with more than 500 reviews')\n",
    "    with open(os.path.join(path_user,str(business_id)+'_with user information.json'),'r',encoding='utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            dict_business_user = json.loads(line)\n",
    "\n",
    "    # save review information into dict_business_raw\n",
    "    dict_business['RestaurantInfo'] = dict_business_user['RestaurantInfo']\n",
    "    dict_business['Reviews'] = {}\n",
    "    for review_id in dict_business_user['Reviews'].keys():\n",
    "        dict_business['Reviews'][review_id] = {}\n",
    "        for attribute in dict_business_user['Reviews'][review_id].keys():\n",
    "            if attribute != 'user_info':\n",
    "                dict_business['Reviews'][review_id][attribute] = dict_business_user['Reviews'][review_id][attribute]\n",
    "\n",
    "    \"\"\" import user features\"\"\"\n",
    "    get_user_features(business_id, path, dict_business)\n",
    "    \"\"\" import text featrues\"\"\"\n",
    "    get_text_features(business_id, path, dict_business)\n",
    "    \"\"\" one-hot for star ratings\"\"\"\n",
    "    one_hot_star(dict_business)\n",
    "\n",
    "    \"\"\" str -> float or datetime\"\"\"\n",
    "    for review_id in dict_business['Reviews']:\n",
    "        for variable in dict_business['Reviews'][review_id].keys():\n",
    "            if variable in list_variables:\n",
    "                dict_business['Reviews'][review_id][variable] = float(dict_business['Reviews'][review_id][variable])\n",
    "            elif variable == 'date':\n",
    "                dict_business['Reviews'][review_id][variable] = datetime.strptime(dict_business['Reviews'][review_id][variable], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    \"\"\" dictionary -> dataframe\"\"\"\n",
    "    df_business = pd.DataFrame.from_dict(dict_business['Reviews']).T\n",
    "    \"\"\" sort by datetime\"\"\"\n",
    "    df_business = df_business.sort_values(by = 'date', axis = 0, ascending = True)\n",
    "\n",
    "    \"\"\" add average_stars\"\"\"\n",
    "    add_average_star(df_business)\n",
    "\n",
    "    \"\"\" clear dataframe\"\"\"\n",
    "    df_business = pd.concat([df_business['date'],df_business['stars'],df_business[list_variables]],axis=1)\n",
    "\n",
    "    \"\"\" calculate decay values\"\"\"\n",
    "    # dictionary for saving effective events (events with decay values > 1e-7)\n",
    "    dict_effective = {}\n",
    "    # dictionary for saving decay values for each event\n",
    "    dict_decayvalues = {}\n",
    "\n",
    "    calculate_decay_values(df_business, list_variables, list_deltas, dict_decaytable, dict_effective, dict_decayvalues)\n",
    "    \n",
    "    \"\"\" get dataframe \"\"\"\n",
    "    # get dataframe of independent variables and save it into .csv for running lasso model in R\n",
    "    df_variables = pd.DataFrame(dict_decayvalues).T.drop('order',axis=1)\n",
    "\n",
    "    \"\"\" b spline basis function \"\"\"\n",
    "    if knot_base == 'order':\n",
    "        # get knot vector\n",
    "        list_knot_order = get_knot_vector(df_business)\n",
    "        # get b spline basis function\n",
    "        df_business, df_bspline = b_spline_basis(bspline_order, list_knot_order, df_business)\n",
    "    elif knot_base == 'time':\n",
    "        # get knot vector\n",
    "        list_knot_order, list_knot_time = get_knot_vector_time(df_business)\n",
    "        # get b spline basis function\n",
    "        df_business, df_bspline = b_spline_basis_time(bspline_order, list_knot_time, df_business)\n",
    "    \n",
    "    # export dataframe\n",
    "    df_data = pd.concat([df_business['stars'],df_variables,df_bspline],axis = 1)\n",
    "    df_data.to_csv(os.path.join(path,'dataframe_ready_to_use' , 'df_'+str(business_id)+'_'+str(knot_base)+'.csv'), index=False, quoting=1)\n",
    "    # export business_id\n",
    "    file = open(os.path.join(path,'dataframe_ready_to_use', 'business_id and knot_base.txt'),'r+')\n",
    "    file.truncate()\n",
    "    file.write(str(business_id)+','+str(knot_base)+'\\n')\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(business_id, knot_base)\n",
    "main(business_id,path,list_deltas,list_variables,bspline_order,knot_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run R for lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "###   Yelp_lasso regression    ###\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "\n",
    "# import library\n",
    "library(glmnet)\n",
    "library(stringr)\n",
    "\n",
    "# setup working directory\n",
    "setwd(\"C:/Users/Yichen Jiang/Documents/PHD LIFE/Research/Hawkes Processes/Yelp/dataframe_ready_to_use\")\n",
    "# read business_id and knot_base\n",
    "parameters <- readLines(\"business_id and knot_base.txt\")\n",
    "business_id = unlist(strsplit(parameters,split = ','))[1]\n",
    "knot_base = unlist(strsplit(parameters,split = ','))[2]\n",
    "# import csv file\n",
    "data_business = read.csv(paste('df_',business_id,'_',knot_base,'.csv',sep = ''))\n",
    "\n",
    "list_variables = colnames(data_business)[which(str_detect(colnames(data_business),'decay')==TRUE)]\n",
    "list_bspline = colnames(data_business)[which(str_detect(colnames(data_business),'spline')==TRUE)]\n",
    "\n",
    "data_business_x = as.matrix(data_business[,c(list_variables,list_bspline)])\n",
    "data_business_y = as.matrix(data_business['stars'])\n",
    "\n",
    "\n",
    "# lasso model with penalty.factor\n",
    "set.seed(2019)\n",
    "\n",
    "# setup penalty factor\n",
    "p.fac = c(rep(1,length(list_variables)),rep(0,length(list_bspline)))\n",
    "\n",
    "lasso =cv.glmnet(data_business_x, data_business_y, penalty.factor = p.fac,alpha = 1, nfolds=5,intercept=FALSE)\n",
    "\n",
    "# get coefficients\n",
    "coef = as.matrix(coef(lasso,s=\"lambda.min\"))\n",
    "print(coef)\n",
    "\n",
    "# get prediction\n",
    "data_business_pred = predict(lasso, data_business_x)\n",
    "\n",
    "# save prediction into data_business\n",
    "data_business[\"prediction\"] = data_business_pred \n",
    "\n",
    "# export data_business coefficients into csv\n",
    "write.csv(data_business, file = paste('df_',business_id,'_',knot_base,'_with predictions.csv',sep = ''),row.names=FALSE)\n",
    "write.csv(coef,file = paste('C:/Users/Yichen Jiang/Documents/PHD LIFE/Research/Hawkes Processes/Yelp/results/',business_id,'_',knot_base,'_coefficients.csv',sep = ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data with predictions and draw plot in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- plot figure ---\n",
    "\n",
    "\"\"\"\n",
    "# import data with prediction values\n",
    "df_data = pd.read_csv(os.path.join(path,'dataframe_ready_to_use' ,'df_'+str(business_id)+'_'+str(knot_base)+'_with predictions.csv'))\n",
    "df_data.index = df_business.index\n",
    "\n",
    "# create datafram to plot and add average star ratings\n",
    "preds = pd.DataFrame({\"preds\":df_data['prediction'], \"true\":df_data['stars']}) \n",
    "preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n",
    "preds[\"average\"] = df_business['average_stars'] \n",
    "\n",
    "# Bspline for smoothing the true values (true star ratings)\n",
    "length = len(preds)\n",
    "index = np.arange(length)\n",
    "y = preds['true']\n",
    "list_year = [list_knot_order[i] for i in range(1,len(list_knot_order)-1)]\n",
    "t,c,k = splrep(index,y,s=0,k=3,t=list_year)\n",
    "prediction_bspline = BSpline(t, c, k, extrapolate=False)\n",
    "index_bspline = np.linspace(index.min(), index.max(), 20000)\n",
    "\n",
    "# plot\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(dpi = 80/2, figsize = (100/2, 50/2))\n",
    "plt.title('Yelp: business_id = '+str(business_id)+', based on '+str(knot_base),  fontdict={'size':'50'})\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.scatter(index, preds['preds'], label='Prediction', color='g',s=25) \n",
    "plt.plot(index_bspline,prediction_bspline(index_bspline), label='True star ratings with b spline function', color='k')\n",
    "#plt.plot(prediction_bspline[0],prediction_bspline[1],label='Prediction by splev',color='k')\n",
    "plt.plot(index, preds['average'], label = 'Average Star Ratings', color = 'r')\n",
    "\n",
    "plt.xlabel('Review Order', fontdict={'size':'50'})\n",
    "plt.ylabel('True Value, Predictions and Mean', fontdict={'size':'50'})\n",
    "plt.xticks(fontsize = 50)\n",
    "plt.yticks(fontsize = 50)\n",
    "plt.axhline(y=preds['average'][len(preds)-1],color='r')\n",
    "#plt.ylim(3.75,5)\n",
    "#plt.xlim(0,1000)\n",
    "plt.legend(loc='upper right',fontsize = 50)\n",
    "plt.savefig(os.path.join(path,'results',str(business_id)+'_'+str(knot_base)+'.png'))\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(business_id)\n",
    "print(knot_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
